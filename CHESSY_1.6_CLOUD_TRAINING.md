# Chessy 1.6 Cloud Training - Free Tier Optimized

## Overview

Chessy 1.6 is now optimized for **Render free tier** with reduced training scope for faster completion.

## What's Optimized

‚úÖ **Reduced Training Scope**
- Games: 500 (was 1000)
- Stockfish depth: 12 (was 15)
- Epochs: 50 (was 100)
- Self-play games: 250 (was 500)
- Test games: 50 (was 100)

‚úÖ **Faster Training**
- Early stopping patience: 5 epochs (was 10)
- Checkpoint interval: 3 epochs (was 5)
- Cross-validation: Disabled for speed
- Max training time: 5 hours (safe for free tier)

‚úÖ **Free Tier Compatible**
- 750 hours/month limit
- Fits in 2-3 hour sessions
- Automatic resume from checkpoints
- No cost

---

## Training Timeline (Free Tier Optimized)

```
Data Generation: 15-20 minutes
Neural Network Training: 1-1.5 hours
Self-Play: 30-45 minutes
Testing: 15-20 minutes
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: 2-3 hours per session
```

---

## Expected Results (Free Tier)

| Metric | Value |
|--------|-------|
| Validation MSE | 0.5 ‚Üí 0.2 |
| Self-Play Win Rate | 50-60% |
| vs Stockfish | 35-45% |
| Estimated ELO | 1600-1900 |

---

## Free Tier Strategy

### Single Session (2-3 hours)
Run once and get a decent trained model (1600-1900 ELO)

### Multiple Sessions (Recommended)
```
Session 1 (Day 1): 2-3 hours
  - Data generation
  - Initial training (20 epochs)
  - Checkpoint saved

Session 2 (Day 2): 2-3 hours
  - Resume from checkpoint
  - Continue training (20 epochs)
  - Self-play
  - Checkpoint saved

Session 3 (Day 3): 2-3 hours
  - Resume from checkpoint
  - Final training (10 epochs)
  - Testing
  - Model saved
```

---

## Quick Deploy

### Step 1: Go to Render
Open [render.com](https://render.com)

### Step 2: Create Background Worker
1. Click "New +" ‚Üí "Background Worker"
2. Select `chessyAI-1.6` repository
3. Configure:
   ```
   Name: chessy-1.6-free
   Build Command: ./build.sh && pip install -r requirements.txt
   Start Command: python3 train_cloud.py
   ```
4. Click "Create Background Worker"

### Step 3: Monitor
- Go to worker dashboard
- Click "Logs"
- Training runs for 2-3 hours
- Checkpoints save automatically

### Step 4: Restart (Optional)
- Click "Manual Deploy"
- Select "Deploy latest commit"
- Training resumes from checkpoint
- Repeat 2-3 times for better results

---

## Configuration

Default config (free tier optimized):
```json
{
  "numGamesGeneration": 500,
  "stockfishDepth": 12,
  "epochs": 50,
  "numSelfPlayGames": 250,
  "numTestGames": 50,
  "overfitting": {
    "enableEarlyStopping": true,
    "patienceEpochs": 5,
    "enableCrossValidation": false
  },
  "maxTrainingHours": 5,
  "checkpointInterval": 3
}
```

---

## Free Tier Limits

| Feature | Free Tier | Limit |
|---------|-----------|-------|
| Hours/Month | 750 | ~25 hours/day |
| Sessions | Unlimited | Restart as needed |
| Cost | $0 | Free |
| Training Time | 2-3 hours | Per session |

---

## Monitoring

### Real-Time Logs
```bash
tail -f training.log
```

### Checkpoints
```bash
ls -la checkpoints/
# checkpoint_epoch_3.bin
# checkpoint_epoch_6.bin
# checkpoint_epoch_9.bin
```

### Final Model
```bash
ls -la models/
# chessy-1.6-trained.bin
```

---

## FAQ

**Q: How long does training take?**
A: 2-3 hours per session on free tier

**Q: Can I run multiple sessions?**
A: Yes! Unlimited restarts within 750 hours/month

**Q: Will I lose progress if it stops?**
A: No! Checkpoints save every 3 epochs

**Q: What's the quality of the trained model?**
A: Good (1600-1900 ELO) for free tier

**Q: Can I upgrade to paid tier?**
A: Yes! Just click "Upgrade" in Render dashboard

**Q: How do I get better results?**
A: Run multiple sessions (2-3 times) for cumulative improvement

---

## Comparison

### Free Tier (This Setup)
- Cost: $0
- Time: 2-3 hours per session
- Quality: Good (1600-1900 ELO)
- Sessions: Multiple needed for best results

### Paid Tier ($7/month)
- Cost: $7/month
- Time: 4-8 hours continuous
- Quality: Excellent (1800-2200 ELO)
- Sessions: Single session for full training

---

## Next Steps

1. Deploy to Render (5 minutes)
2. Monitor first session (2-3 hours)
3. Download model or restart for better results
4. Use trained model in your chess engine

**Cost: $0** üéâ

---

## Support

For issues:
1. Check Render logs
2. Review troubleshooting section
3. See `RENDER_FREE_TIER_GUIDE.md` for detailed guide

---

**Status**: ‚úÖ Optimized for Render free tier
**Training Time**: 2-3 hours per session
**Cost**: $0
**Quality**: Good (1600-1900 ELO)

---

## Quick Start: Deploy to Render

### Step 1: Prepare Your Repository

```bash
cd chessy-1.6
git add .
git commit -m "Add cloud training support"
git push origin main
```

### Step 2: Create Render Account

1. Go to [render.com](https://render.com)
2. Sign up with GitHub
3. Authorize Render to access your repositories

### Step 3: Deploy Training Worker

1. Click **"New +"** ‚Üí **"Background Worker"**
2. Select your repository
3. Configure:
   ```
   Name: chessy-1.6-trainer
   Runtime: Docker
   Build Command: ./build.sh && pip install -r requirements.txt
   Start Command: python3 train_cloud.py
   ```
4. Click **"Create Background Worker"**

### Step 4: Monitor Training

1. Go to your worker dashboard
2. Click **"Logs"** to see real-time training output
3. Training will run continuously for 24 hours
4. Checkpoints save every 5 epochs

---

## Local Testing

### Test the Training Pipeline

```bash
# Build the project
./build.bat  # Windows
./build.sh   # Linux/macOS

# Run training locally
python3 train_cloud.py

# Or directly
./bin/chessy-1.6 --train
```

### Monitor Progress

```bash
# Watch checkpoints being created
ls -la checkpoints/

# View training logs
tail -f training.log
```

---

## Overfitting Prevention Details

### Early Stopping
- **Patience**: 10 epochs without improvement
- **Threshold**: 0.001 minimum improvement
- **Benefit**: Prevents wasting compute on plateaued training

### Dropout
- **Rate**: 30% of features randomly dropped
- **Effect**: Forces network to learn robust features
- **Applied**: During both training and validation

### L2 Regularization
- **Weight**: 0.0001
- **Effect**: Penalizes large weights
- **Benefit**: Prevents overfitting to training data

### Data Augmentation
- **Rate**: 20% of training data
- **Method**: Add ¬±5 centipawn noise to evaluations
- **Benefit**: Increases training data diversity

### Validation Strategy
- **Split**: 80% training, 20% validation
- **K-Fold**: 5-fold cross-validation support
- **Monitoring**: Validation loss tracked every epoch

---

## Configuration

Edit `training_config.json` to customize:

```json
{
  "numGamesGeneration": 1000,      // More games = better training
  "stockfishDepth": 15,             // Deeper = stronger but slower
  "blunderRate": 0.05,              // 5% blunder injection
  "epochs": 100,                    // Max epochs (early stop may end earlier)
  "learningRate": 0.001,            // Lower = slower but more stable
  "overfitting": {
    "enableEarlyStopping": true,
    "patienceEpochs": 10,           // Stop after 10 epochs no improvement
    "dropoutRate": 0.3,             // 30% dropout
    "l2Regularization": 0.0001,
    "enableDataAugmentation": true,
    "augmentationRate": 0.2         // Augment 20% of data
  },
  "isCloudDeployment": true,
  "maxTrainingHours": 24            // Stop before timeout
}
```

---

## Checkpointing System

### How It Works

1. **Automatic Saves**: Every 5 epochs
2. **Location**: `./checkpoints/checkpoint_epoch_N.bin`
3. **Resume**: Automatically loads latest checkpoint on restart
4. **Safety**: Never loses progress

### Manual Checkpoint Management

```bash
# List all checkpoints
ls -la checkpoints/

# Load specific checkpoint
./bin/chessy-1.6 --load-checkpoint checkpoints/checkpoint_epoch_50.bin

# Clean old checkpoints (keep last 5)
ls -t checkpoints/checkpoint_epoch_*.bin | tail -n +6 | xargs rm
```

---

## Deployment Platforms

### Render (Recommended)
- **Cost**: Free tier available
- **Setup**: 5 minutes
- **Pros**: Simple, GitHub integration, auto-deploy
- **Cons**: Free tier has 750 hours/month limit

### AWS EC2
- **Cost**: ~$0.05/hour (t3.micro)
- **Setup**: 15 minutes
- **Pros**: Scalable, reliable
- **Cons**: More complex setup

### DigitalOcean
- **Cost**: $5-40/month
- **Setup**: 10 minutes
- **Pros**: Simple, affordable
- **Cons**: Manual management

### Heroku
- **Cost**: $7/month (paid dyno)
- **Setup**: 5 minutes
- **Pros**: Simple, GitHub integration
- **Cons**: Expensive for long training

---

## Monitoring & Logs

### Real-Time Monitoring

```bash
# Watch training progress
tail -f training.log

# Check checkpoint creation
watch -n 5 'ls -la checkpoints/ | tail -5'

# Monitor system resources
top
```

### Log Analysis

```bash
# Extract training metrics
grep "Epoch" training.log

# Find early stopping events
grep "Early stopping" training.log

# Check validation loss trend
grep "Validation MSE" training.log
```

---

## Troubleshooting

### Training Stops Unexpectedly

**Check logs for:**
```bash
grep "ERROR\|FATAL" training.log
```

**Common issues:**
- Out of memory: Reduce `numGamesGeneration`
- Stockfish not found: Install or provide path
- Time limit exceeded: Increase `maxTrainingHours`

### Validation Loss Not Improving

**Possible causes:**
- Learning rate too high: Reduce to 0.0005
- Learning rate too low: Increase to 0.002
- Insufficient data: Increase `numGamesGeneration`
- Model capacity: Increase hidden layer sizes

### Checkpoints Not Saving

**Check:**
```bash
ls -la checkpoints/
```

**If empty:**
- Verify write permissions
- Check disk space: `df -h`
- Ensure `enableCheckpointing: true`

---

## Performance Tips

### Faster Training
1. Reduce `stockfishDepth` to 12
2. Reduce `numGamesGeneration` to 500
3. Increase `learningRate` to 0.002
4. Disable `enableDataAugmentation`

### Better Results
1. Increase `numGamesGeneration` to 2000
2. Increase `stockfishDepth` to 18
3. Increase `epochs` to 200
4. Enable `enableCrossValidation`

### Balanced Setup
- `numGamesGeneration`: 1000
- `stockfishDepth`: 15
- `epochs`: 100
- `learningRate`: 0.001
- All overfitting prevention enabled

---

## Expected Results

### Training Timeline
- **Data Generation**: 30-60 minutes
- **Neural Network Training**: 2-4 hours
- **Self-Play**: 1-2 hours
- **Testing vs Stockfish**: 30 minutes
- **Total**: 4-8 hours per full cycle

### Performance Metrics
- **Validation MSE**: Should decrease from ~0.5 to ~0.1
- **Self-Play Win Rate**: Target 55-65%
- **vs Stockfish**: Target 40-50% (depth 10)
- **Estimated ELO**: 1800-2200

---

## Advanced: Custom Training

### Modify Training Config

```cpp
// In trainer.cpp
TrainingConfig config;
config.numGamesGeneration = 2000;  // More data
config.stockfishDepth = 18;        // Stronger analysis
config.epochs = 200;               // More training
config.overfitting.dropoutRate = 0.4;  // More regularization
```

### Add Custom Evaluation

```cpp
// In trainer.cpp, modify testVsStockfish()
// Add your own evaluation metrics
```

---

## FAQ

**Q: Will training continue if my computer is off?**
A: Yes! Training runs on the cloud server, not your computer.

**Q: How much does it cost?**
A: Render free tier: $0. Paid tier: $7/month. AWS: ~$1.20/day.

**Q: Can I stop training and resume later?**
A: Yes! Checkpoints save automatically. Just restart the worker.

**Q: How do I use the trained model?**
A: The model saves to `models/chessy-1.6-trained.bin`. Load it with:
```cpp
trainer.loadModel("models/chessy-1.6-trained.bin");
```

**Q: What if training gets interrupted?**
A: Latest checkpoint loads automatically on restart. No progress lost.

**Q: Can I train multiple instances?**
A: Yes, but they'll overwrite each other's models. Use different output paths.

---

## Next Steps

1. ‚úÖ Build the project locally
2. ‚úÖ Test training with `python3 train_cloud.py`
3. ‚úÖ Push to GitHub
4. ‚úÖ Deploy to Render
5. ‚úÖ Monitor training progress
6. ‚úÖ Download trained model when complete

---

## Support

For issues:
1. Check `training.log` for errors
2. Review this guide's troubleshooting section
3. Check Render/cloud provider logs
4. Verify Stockfish is installed

Happy training! üöÄ‚ôüÔ∏è
